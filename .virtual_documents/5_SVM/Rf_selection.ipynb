!pip install biopython
!pip install aaindex


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys
import sklearn.metrics as skl
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')
from Bio.SeqUtils.ProtParam import ProteinAnalysis
from Bio.SeqUtils import ProtParamData
from aaindex import aaindex1
from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score





# Import the training set
training = pd.read_csv("../2_data_preparation/sets/train_with_seq.tsv", sep='\t')
training.head()


benchmark = pd.read_csv("../2_data_preparation/sets/test_with_seq.tsv", sep='\t')
benchmark.head()


# Assign a numeric index to each aminoacid
order = list("ARNDCQEGHILKMFPSTWYV")
index_aa = {}
for i, aa in enumerate(order):
  index_aa[aa]= i
print(index_aa)


def aa_composition(sequence, length: int, index_aa):
  ''' Calculate the aminoacid frequency of a sequence of a given length
  '''
  sequence = sequence[:length]
  freq = np.zeros((1, 20))
  for aa in sequence:
    if aa in index_aa.keys():
      freq[0,index_aa[aa]] += 1
  freq = np.round(freq/len(sequence), 3)
  return freq

# Calculate the aminoacid composition for each sequence of the dataframe and save it into a unique numpy matrix
aa_comp = np.vstack(training['sequence'].apply(aa_composition, args=(22, index_aa)))
aa_comp_test = np.vstack(benchmark['sequence'].apply(aa_composition, args=(22, index_aa)))


def hydrophobicity(sequence, window: int, length: int):
  ''' Calculates the hydrophobicity of each residue considering the context given by sliding window of adiacent aminoacids
  '''
  sequence = sequence[:length]
  seq = ProteinAnalysis(sequence)
  kd_pos = seq.protein_scale(ProtParamData.kd,window)
  d = int(window/2)
  sequence_with_padding = "X"*d + sequence + "X"*d
  seq_padding = ProteinAnalysis(sequence_with_padding)
  kd_pos_with_padding = seq_padding.protein_scale(ProtParamData.kd, window)
  warnings.filterwarnings('ignore')
  return kd_pos_with_padding

# Calculate the hydrophobicity of the first 40 residues for each sequence and save it into a unique matrix numpy
hydro = np.vstack(training['sequence'].apply(hydrophobicity, args=(5,40)))
hydro_test = np.vstack(benchmark['sequence'].apply(hydrophobicity, args=(5,40)))


feature_codes = {
    "net_charge": ("KLEP840101", 2),
    "hydrophilicity": ("HOPT810101", 5),
    "helix_propensity": ("CHAM830101",7),
    "flexibility": ("BHAR880101", 7),
    "isoelectric_point": ("ZIMJ680104", 2),
    "bulkiness": ("ZIMJ680102", 7)
}

def features(sequence, length: int, feature_code, window: int):
  ''' Creates an array of 3 values corresponding to mean, standard deviation,
      and maximum value of a given dictionary of features
  '''
  sequence = sequence[:length]
  seq = ProteinAnalysis(sequence)
  vals = aaindex1[feature_code].values
  val = seq.protein_scale(vals,window)
  mean = np.mean(val)
  std = np.std(val)
  max = np.max(val)
  return np.round([mean, std, max], 3)

feats = {}
for feature in feature_codes.keys():
  feats[feature] = np.vstack(training['sequence'].apply(features, args=(30, feature_codes[feature][0], feature_codes[feature][1])))

test_feat = {}
for feature in feature_codes.keys():
  test_feat[feature] = np.vstack(benchmark['sequence'].apply(features, args=(30, feature_codes[feature][0], feature_codes[feature][1])))

# Create the
np.savez('separate_features.npz', aa_comp = aa_comp, hydrophobicity = hydro, **feats)
np.savez('test_features.npz', aa_comp= aa_comp_test, hydrophobicity = hydro_test, **test_feat)


# Create a dictionary from the loaded features, handling multi-dimensional arrays
def npz_to_dataframe(npz_file):
  extracted_features = np.load(npz_file)
  data = {}
  for key in extracted_features.files:
      arr = extracted_features[key]
      if arr.ndim > 1:
          # If the array is multi-dimensional, create multiple columns
          for i in range(arr.shape[1]):
              data[f'{key}_{i+1}'] = arr[:, i]
      else:
          # If the array is 1-dimensional, use it directly
          data[key] = arr
  X = pd.DataFrame(data)
  return X, data

X, data = npz_to_dataframe("separate_features.npz")
X.head()


T = npz_to_dataframe("test_features.npz")[0]


X["id"] = training["id"]
X["validation_n"] = training["validation_n"]
y = training["sp_type"]
X.head()



Y_T = benchmark["sp_type"]





import sys
sys.path.append('../shared_code')  # aggiungi la cartella al path

import Von_Heijne_with_benchmark as vh


groups = vh.make_groups()
print(g)


def svm_pipeline(C, gamma):
    return Pipeline([
        ("scaler", StandardScaler()),
        ("svm", SVC(kernel="rbf", C=C, gamma=gamma, random_state=42))
    ])

def accuracy_on_subset(C, gamma, subset_features):
    # subset by feature names
    # Convert data keys to a NumPy array for use with np.where
    data_keys_np = np.array(list(data.keys()))
    idx = [np.where(data_keys_np == f)[0][0] for f in subset_features]
    Xtr = X_train.iloc[:, idx]
    Xva = X_val.iloc[:, idx]
    pipe = svm_pipeline(C, gamma)
    pipe.fit(Xtr, y_train)     # train on TRAIN only
    return pipe.score(Xva, y_val)  # accuracy on VALIDATION



def grid_search(C_grid, gamma_grid, X_train, Y_train, X_val, Y_val):
    best_score = -np.inf
    best_params = None

    for C in C_grid:
        for gamma in gamma_grid:
            pipe = svm_pipeline(C, gamma)
            pipe.fit(X_train, Y_train)
            val_acc = pipe.score(X_val, Y_val)
            if val_acc > best_score:
                best_score = val_acc
                best_params = {"C": C, "gamma": gamma}

    return best_score, best_params


results = []
# Define search grids for SVM hyperparameters C and gamma.
C_grid = [0.1, 1.0, 10.0, 100.0]
gamma_grid = ["scale", 0.01, 0.1, 1.0]

all_metric_sel = []
all_metric_all = []

# Each iteration corresponds to one cross-validation fold.
# For each fold, we define TRAIN, VALIDATION, and TEST sets based on validation_n values.
for name, (train_idx, val_idx, test_idx) in groups.items():
    print(f"\n=== {name.upper()} ===")
    print(f"Train: {train_idx}, Val: {val_idx}, Test: {test_idx}")

    # Filter the main dataset (X and y) according to the fold indices.
    # Drop non-numerical or identifier columns such as 'id' and 'validation_n'.
    X_train = X[X["validation_n"].isin(train_idx)].drop(columns=["id", "validation_n"])
    X_val   = X[X["validation_n"].isin([val_idx])].drop(columns=["id", "validation_n"])
    X_test  = X[X["validation_n"].isin([test_idx])].drop(columns=["id", "validation_n"])

    y_train = y[training["validation_n"].isin(train_idx)]
    y_val   = y[training["validation_n"].isin([val_idx])]
    y_test  = y[training["validation_n"].isin([test_idx])]

    # Convert the DataFrames into NumPy arrays and save them in a single .npz file.
    # This stores all matrices for the current fold (train, validation, test) in one place.

    X_train_np = X_train.to_numpy()
    X_val_np = X_val.to_numpy()
    X_test_np = X_test.to_numpy()

    y_train_np = y_train.to_numpy()
    y_val_np = y_val.to_numpy()
    y_test_np = y_test.to_numpy()

    np.savez(
        f"cv_set_{name}.npz",
        X_train=X_train_np,
        X_val=X_val_np,
        X_test=X_test_np,
        y_train=y_train_np,
        y_val=y_val_np,
        y_test=y_test_np
    )

    # Perform a manual grid search over C and gamma values.
    # Evaluate each model on the VALIDATION set and store the best parameters.

    best_score_base, best_params_base = grid_search(
    C_grid, gamma_grid,
    X_train.to_numpy(), y_train.to_numpy(),
    X_val.to_numpy(), y_val.to_numpy())
    print( f"{best_score_base:.3f} with params {best_params_base}")


    # Train a Random Forest classifier on the training data.
    # Use the Gini importance to rank features by their predictive power.
    rf = RandomForestClassifier(n_estimators=400,random_state=42,n_jobs=-1)
    rf.fit(X_train, y_train)

    gini_imp = pd.Series(rf.feature_importances_, index=list(data.keys())).sort_values(ascending=False)
    gini_df = gini_imp.reset_index()
    gini_df.columns = ["feature", "importance"]
    print("Top 10 features by Gini importance:")
    print(gini_df.head(10))

    # Visualize the 20 most important features as determined by the Random Forest.
    # Save the figure for documentation of this fold.
    plt.figure()
    plt.barh(gini_df["feature"].head(20)[::-1], gini_df["importance"].head(20)[::-1])
    plt.xlabel("Gini importance")
    plt.ylabel("Feature")
    plt.title(f"RandomForest Gini Importances (Top 20) - {name}")
    plt.savefig(f"gini_importances_{name}.png", dpi=300, bbox_inches="tight")
    plt.show()

    # Test multiple subsets of top-k features ranked by Gini importance.
    # For each subset size, evaluate the validation accuracy of the baseline SVM.
    ks = list(range(2, min(26, X_train.shape[1]+1)))  # keep it small for speed/clarity
    curve = []

    for k in ks:
        subset = gini_df["feature"].head(k).tolist()
        acc_k = accuracy_on_subset(best_params_base["C"], best_params_base["gamma"], subset)
        curve.append(acc_k)

    best_k_idx = int(np.argmax(curve))
    best_k = ks[best_k_idx]
    print(f"Best k on validation (using baseline best params): k={best_k}, val_acc={curve[best_k_idx]:.3f}")

    # Visualize how validation accuracy changes with different numbers of features.
    # The best k will correspond to the peak of the curve.
    plt.figure()
    plt.plot(ks, curve, marker="o")
    plt.xlabel("k (top features by RF Gini)")
    plt.ylabel("Validation accuracy (SVM)")
    plt.title(f"Accuracy vs Number of Selected Features ({name})")
    plt.grid(True)
    plt.savefig(f"accuracy_vs_features_{name}.png", dpi=300, bbox_inches="tight")
    plt.show()


    # Keep only the best_k features and re-run the grid search to find new optimal parameters.
    best_subset = gini_df["feature"].head(best_k).tolist()
    data_keys_np = np.array(list(data.keys()))
    idx = [np.where(data_keys_np == f)[0][0] for f in best_subset]

    Xtr_sel = X_train.iloc[:, idx]
    Xva_sel = X_val.iloc[:, idx]
    Xte_sel = X_test.iloc[:, idx]

    best_score_sel, best_params_sel = grid_search(
    C_grid, gamma_grid,
    Xtr_sel.to_numpy(), y_train.to_numpy(),
    Xva_sel.to_numpy(), y_val.to_numpy())
    final_pipe = svm_pipeline(best_params_sel["C"], best_params_sel["gamma"])
    final_pipe.fit(Xtr_sel.to_numpy(), y_train.to_numpy())
    test_acc = final_pipe.score(Xte_sel.to_numpy(), y_test.to_numpy())

    # For comparison: test accuracy with all features using baseline best params
    baseline_pipe = svm_pipeline(best_params_base["C"], best_params_base["gamma"])
    baseline_pipe.fit(X_train.to_numpy(), y_train.to_numpy())
    test_acc_all = baseline_pipe.score(X_test.to_numpy(), y_test.to_numpy())


    print("\n--- RESULTS SUMMARY ---")
    print(f"Best validation accuracy (after feature selection): {best_score_sel:.3f}")
    print(f"   → Found with parameters: C = {best_params_sel['C']}, gamma = {best_params_sel['gamma']}")

    print(f"\nFinal test accuracy (using selected features): {test_acc:.3f}")
    print(f"Final test accuracy (using all features - baseline model): {test_acc_all:.3f}")

    if test_acc > test_acc_all:
      print("\n The model with feature selection performs slightly better on the test set.")
    elif test_acc == test_acc_all:
      print("\n The model with selected features performs the same as the baseline.")
    else:
      print("\n The baseline model (all features) performs slightly better on the test set.")


    #METRICS
    y_pred_sel = final_pipe.predict(Xte_sel)
    y_pred_all = baseline_pipe.predict(X_test)

    obs_test = y_test

    MCC_sel, ACC_sel, PPV_sel, SEN_sel, CONF_sel = vh.metrics(obs_test, y_pred_sel)
    all_metric_sel.append([MCC_sel, ACC_sel,  PPV_sel, SEN_sel])

    MCC_all, ACC_all, PPV_all, SEN_all, CONF_all = vh.metrics(obs_test, y_pred_all)
    all_metric_all.append([MCC_all, ACC_all, PPV_all, SEN_all])

    print("\n=== SVM with FEATURE SELECTION ===")
    print(f"MCC = {MCC_sel:.3f} | ACC = {ACC_sel:.3f} | PPV = {PPV_sel:.3f} | SEN = {SEN_sel:.3f}")
    print("Confusion Matrix:\n", CONF_sel)

    print("\n=== SVM with ALL FEATURES (BASELINE) ===")
    print(f"MCC = {MCC_all:.3f} | ACC = {ACC_all:.3f} | PPV = {PPV_all:.3f} | SEN = {SEN_all:.3f}")
    print("Confusion Matrix:\n", CONF_all)

    results = pd.DataFrame({
    'Model': ['Selected features', 'All features'],
    'MCC': [MCC_sel, MCC_all],
    'ACC': [ACC_sel, ACC_all],
    'PPV': [PPV_sel, PPV_all],
    'SEN': [SEN_sel, SEN_all]
    })

    print(results)


#msd for the metric with all the features
arr_all = np.array(all_metric_all)
mean_all = np.mean(arr_all, axis=0)
std_all = np.std(arr_all, axis=0)

#mean and st for the metrics with selected features
arr_sel = np.array(all_metric_sel)
mean_sel = np.mean(arr_sel, axis=0)
std_sel = np.std(arr_sel, axis=0)

print("\n=== FEATURE SELECTION ===")
for name, mean, se in zip(["MCC","ACC","PPV","SEN"], mean_sel, std_sel):
    print(f"{name} = {mean:.3f} ± {se:.3f}")

print("\n=== ALL FEATURES ===")
for name, mean, se in zip(["MCC","ACC","PPV","SEN"], mean_all, std_all):
    print(f"{name} = {mean:.3f} ± {se:.3f}")





train_indexes = [1,2,3,4]
x_training = X[X['validation_n'].isin(train_indexes)].drop(columns=["id", "validation_n"])
y_training = training["sp_type"][training['validation_n'].isin(train_indexes)]
x_validation = X[X['validation_n']==5].drop(columns=["id", "validation_n"])
y_validation = training["sp_type"].loc[training['validation_n']==5]


best_score, best_params = grid_search(C_grid, gamma_grid, x_training.to_numpy(), y_training.to_numpy(),x_validation.to_numpy(), y_validation.to_numpy())
final_pipeline = svm_pipeline(best_params["C"], best_params["gamma"])
final_pipeline.fit(x_training.to_numpy(), y_training.to_numpy())
test_acc = final_pipeline.score(T.to_numpy(), Y_T.to_numpy())
print( f"{best_score:.3f} with params {best_params}")



test_metrics = []
y_predictions = final_pipeline.predict(T)

observed_test = Y_T

mcc, acc, ppv, sen, conf = vh.metrics(observed_test, y_predictions)
test_metrics.append([mcc, acc, ppv, sen])

print("\n=== SVM on BENCHMARKING SET ===")
print("Confusion Matrix:\n", conf)

res = pd.DataFrame({
'MCC': mcc,
'ACC': acc,
'PPV': ppv,
'SEN': sen
}, index=[0])

print(res)


cm = np.array(conf)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[0,1],
            yticklabels= [0,1])

plt.title("Confusion Matrix – SVM Benchmark")
plt.xlabel("Predicted labels")
plt.ylabel("Real labels")
plt.tight_layout()
plt.show()


y_scores = final_pipeline.decision_function(T.to_numpy())

fpr, tpr, thresholds = roc_curve(Y_T, y_scores)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM ROC Curve – Benchmark Set')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()


y_scores = final_pipeline.decision_function(T.to_numpy())
precision, recall, thresholds = precision_recall_curve(Y_T, y_scores)
avg_precision = average_precision_score(Y_T, y_scores)
plt.figure()
plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision–Recall Curve per SVM')
plt.legend(loc='best')
plt.grid(True)
plt.show()

